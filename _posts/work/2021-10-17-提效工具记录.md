---
layout: post
title: 提效工具记录
date: 2021-10-17
tags: interview
---  
# hive sql相关
## 常见错误
##### hive msck repair table 报错
```shell
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
```
处理办法：
set hive.msck.path.validation=ignore;
msck repair table 库名.表名

##### 数据串列
```sql
--临时表使用orc格式存储
create table tmp_table stored as orc as select ...
--可替换特殊字符后使用
regexp_replace(room_name,'\\n|\\t|\\r', '') as room_name
```

##### hadoop Unexpected end of input stream 错误
```shell
错误信息：

2021-05-31 10:57:57,333 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: java.io.IOException: java.io.EOFException: Unexpected end of input stream
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:231)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.next(HadoopShimsSecure.java:141)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:199)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:185)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.io.IOException: java.io.EOFException: Unexpected end of input stream
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:106)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:42)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:228)
	... 11 more
Caused by: java.io.EOFException: Unexpected end of input stream
	at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:145)
	at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)
	at java.io.InputStream.read(InputStream.java:101)
	at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:180)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)
	... 15 more
对应mapper中读取文件的路径

2021-05-31 10:57:56,103 INFO [main] org.apache.hadoop.mapred.MapTask: Processing split: Paths:/user/qhstats/ods/ods_hotel_log_breeze/dt=20210528/hour=21/h_hspa_l-qtaspaphy15.h.cn2_out.product.log.2021-05-28-21.gz:0+115093InputFormatClass: org.apache.hadoop.mapred.TextInputFormat
从报错信息来看，是和io读取有关系的，即在map阶段，数据读取出错导致。

通过explain extended查看读取的文件，因为是text的gz文件，使用zcat进行测试，最终定位到是由于gz文件异常导致。将有问题的数据删除后job恢复。
```

## 正则匹配
##### 提取字符串中指定数据
coalesce(regexp_extract(params, 'uid=(.*?)&', 1), '') as uid
最小匹配（.*?）
最大匹配（.*）

##### 反向引用
捕获会返回一个捕获组，这个分组是保存在内存中的，不仅可以在正则表达式外部通过程序进行引用，也可以在正则表达式内部进行引用，这种引用方式就是反向引用。
根据捕获组的命名规则，反向引用可分为：
1.数字编号组反向引用：\k或\number
2.命名编号组反向引用：\k或\‘name’
捕获组是匹配子表达式的内容按序号或者命名保存起来以便使用，主要是用来查找一些重复的内容或者替换指定字符。
select 'abac' regexp '(\\w)(\\w)\\1\\2'
select regexp_replace('ababcccdcd','(\\w)(\\w)\\1\\2','')
select regexp_extract('ababcccdcd','(\\w)(\\w)\\1\\2',0)

##### 匹配所有叠词
如：haha，123123，12341234
select '123123' regexp '(\\w{1,})\\1';

## 参数说明
##### 调优参数
```shell
set hive.map.aggr=true;
# 是否开启mapper端聚合
hive.map.aggr
# 是否开启，如果数据倾斜，是否优化group by为两个MR job
#该配置会触发hive增加额外的mr过程，随机化key后进行聚合操作得到中间结果，再对中间结果执行最终的聚合操作。
#count(distinct)操作比较特殊，无法进行中间的聚合操作，因此该参数对有count(distinct)操作的sql不适用。

set hive.groupby.skewindata=true;

#MAPJOIN
#  select /*+ MAPJOIN(supplier, room, ab) */
set hive.auto.convert.join=true;
set hive.optimize.skewjoin=true;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions.pernode=100000;

# Error: Java heap space | Error: GC overhead limit exceeded | FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
# mapreduce.map.memory.mb 一个Map Task可使用的资源上限（单位:MB），默认为1024。如果Map Task实际使用的资源量超过该值，则会被强制杀死
# mapreduce.map.java.opts 设置 jvm heap size
set mapreduce.map.memory.mb=8192;
set mapreduce.map.java.opts=-Xmx6553M;
set mapreduce.reduce.memory.mb=8192;
set mapreduce.reduce.java.opts=-Xmx6553M;

# Error: Java heap space Container killed by the ApplicationMaster. Container killed on request. Exit code is 143 Container exited with a non-zero exit code 143
set mapred.child.java.opts=-Xmx512M;
# io.sort.mb 的作用 排序所使用的内存数量。  默认值：100M，需要与mapred.child.java.opts相配 默认：-Xmx200m。  不能超过mapred.child.java.opt设置，否则会OOM。
set io.sort.mb=50;

set mapreduce.map.output.compress=true;
set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;
set hive.exec.compress.intermediate=true;

set hive.exec.compress.output=true;
set mapreduce.output.fileoutputformat.compress=true
set mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzopCodec;
set mapreduce.output.fileoutputformat.compress.type=BLOCK

set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set mapreduce.input.fileinputformat.split.maxsize=512000000;
set mapreduce.input.fileinputformat.split.minsize.per.node=64000000;
set mapreduce.input.fileinputformat.split.minsize.per.rack=64000000;

set mapreduce.reduce.input.buffer.percent=1;

set mapred.job.priority=HIGH;
#增加reduce数
set mapred.reduce.tasks = 100;
set hive.exec.reducers.bytes.per.reducer=250000000;
#增加map数
set mapred.max.split.size=52428800; -- 决定每个map处理的最大的文件大小，单位为B
set mapred.min.split.size.per.node=1; -- 节点中可以处理的最小的文件大小
set mapred.min.split.size.per.rack=1; -- 机架中可以处理的最小的文件大小

set mapred.reduce.tasks = 10; -- 设置reduce的数量


#禁用mapjoin
set hive.auto.convert.join=false;  --禁用自动MapJoin
set hive.ignore.mapjoin.hint=false;  --禁用自动检测


#error in shuffle in fetcher#3
set mapreduce.reduce.shuffle.memory.limit.percent=0.15;
```

## sql无理要求

##### 数字前补0，eg：0->00
```sql
--0->00
substr(cast(power(10,2)+log_hour as string),2,2)
```

##### 字符串数组array转array
```sql
--["abc","ef"]
select sort_array(split(regexp_replace('["abc","ef"]','^\\[|\\]$|\\"',''),','))
```

##### 字符串map转map
```sql
--{user_ip=58.222.204.210, qn1=null,qn2=, inner_channel=C2075}
select str_to_map(regexp_replace('{user_ip=58.222.204.210, qn1=null,qn2=, inner_channel=C2075}', '^\\{|\\}$', ''),  ',' ,'=')
```

###### map类型展开
| 字段名    |  类型    |  说明    |     
| :--------: | :-----: | :----: |      
|init_subtract_detail|map<string,decimal(18,4)>|直接金额优惠明细(下单初始值) key定义为优惠主题名称，具体主题名字详解:返现，直减主题对应表|
LATERAL VIEW explode(init_subtract_detail) t as myMapKey,myMapValue

# shell相关
## alias节省时间

## 无脑循环刷数

## screen后台执行
